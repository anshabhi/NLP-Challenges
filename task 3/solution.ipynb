{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "225108d0-898e-9e00-5b7f-331df096a7d0"
   },
   "source": [
    "This is my submission for the task Tagging System of Questions using Transfer Learning. We use a kind of brute force method, using TF-IDF to predict the best possible tags for a question.\n",
    "\n",
    "\n",
    "First, load the required libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "60e5123b-0d2e-faff-5148-b6246aa9c8d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biology.csv\n",
      "cooking.csv\n",
      "crypto.csv\n",
      "diy.csv\n",
      "robotics.csv\n",
      "sample_submission.csv\n",
      "test.csv\n",
      "travel.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import nltk # natural language processing\n",
    "import re # regular expression\n",
    "from bs4 import BeautifulSoup #scraping HTML\n",
    "from nltk.corpus import stopwords\n",
    "import seaborn as sns # visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from string import punctuation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# nltk workspace\n",
    "\n",
    "stop = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "f81d7e82-fd33-30dc-6d75-e049fa9aff96"
   },
   "outputs": [],
   "source": [
    "# read in test data sets\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "17b1d0a6-d27c-d3d9-2868-fea5a034d3f9"
   },
   "source": [
    "# **Data Cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a2fe2f86-f49b-6eb9-e4b4-bbb9fa769760"
   },
   "source": [
    "The content column contains some HTML tags. We clean our data to remove these tags, convert all to small case letters and remove stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "3727ba05-f6f4-495e-257f-0a77c3344240"
   },
   "outputs": [],
   "source": [
    "def parse_content(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    #text = # lowercase text\n",
    "    text =text.lower()\n",
    "    #text = # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = re.sub(REPLACE_BY_SPACE_RE, ' ', text)\n",
    "    #text = # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text =  re.sub(BAD_SYMBOLS_RE, '', text)\n",
    "\ttext = re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "\ttext = re.sub(\"\\d+\", \"\", text)\n",
    "\n",
    "    #text = # delete stopwords from text\n",
    "    token_word=word_tokenize(text)\n",
    "    filtered_sentence = [w for w in token_word if not w in STOPWORDS] # filtered_sentence contain all words that are not in stopwords dictionary\n",
    "    lenght_of_string=len(filtered_sentence)\n",
    "    text_new=\"\"\n",
    "    for w in filtered_sentence:\n",
    "        if w!=filtered_sentence[lenght_of_string-1]:\n",
    "             text_new=text_new+w+\" \" # when w is not the last word so separate by whitespace\n",
    "        else:\n",
    "            text_new=text_new+w\n",
    "            \n",
    "    text = text_new\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "df4f6289-3856-8c5b-aff7-81953567f561"
   },
   "outputs": [],
   "source": [
    "# Apply parse_content onto dataframe.\n",
    "raw = pd.concat([raw,raw['content'].apply(parse_content)],axis = 1)\n",
    "raw = pd.concat([raw,raw['title'].apply(parse_content)],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7cc8261c-5d03-154a-81c5-6c28ba540d70"
   },
   "source": [
    "We do further processing on our text by selecting nouns which match a certain patter, and converting the entire document into tokens of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "c4987560-82df-5625-b7bf-d960c3543304"
   },
   "outputs": [],
   "source": [
    "#tokenize and tag texts. \n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "raw['all_text'] = raw['title'] + raw['content']\n",
    "raw['text_token'] = raw['all_text'].apply(nltk.word_tokenize)\n",
    "raw['text_token'] = raw['text_token'].apply(lambda x:[lemmatizer.lemmatize(t) for t in x])\n",
    "raw['text_pos'] = raw['text_token'].apply(nltk.pos_tag)\n",
    "raw['text_nouns'] = raw['text_pos'].apply(lambda x: [pair[0] for pair in x if pair[1] in (\"NN\",\"NNS\",\"JJ\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "e6c3f779-8c69-8570-0ea9-26b707b2931e"
   },
   "outputs": [],
   "source": [
    "raw['text_bigram'] = raw['text_pos'].apply(nltk.bigrams)\n",
    "raw['text_bigram'] = raw['text_bigram'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "ee749d56-1533-a076-f249-6d581d0de057"
   },
   "outputs": [],
   "source": [
    "def findPair(l):\n",
    "    result = []\n",
    "    for pair in l:\n",
    "        if pair[1][1] in ('NN','NNS') and pair[0][1] in ('NN','NNS','JJ'):\n",
    "            result.append(pair[0][0]+\" \"+pair[1][0])\n",
    "    return result\n",
    "raw['word_pair'] = raw['text_bigram'].apply(findPair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "597f9306-cfb6-b826-fd0e-8905551d3424"
   },
   "source": [
    "Now, lets make the actual predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "c856254e-b879-6250-9947-3ed95bc2ec53"
   },
   "outputs": [],
   "source": [
    "mydoclist = raw['text_nouns'].apply(\" \".join).tolist()\n",
    "#mydoclist[0:5]\n",
    "count_vectorizer = CountVectorizer(stop_words='english',lowercase=True,analyzer='word',ngram_range=(1,1))\n",
    "term_freq_matrix = count_vectorizer.fit_transform(mydoclist)\n",
    "tfidf = TfidfTransformer(norm=\"l2\")\n",
    "tfidf.fit(term_freq_matrix)\n",
    "tf_idf_matrix = tfidf.transform(term_freq_matrix)\n",
    "pos_to_word = dict([[v,k] for k,v in count_vectorizer.vocabulary_.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "03354ab4-c6ed-98d2-e5ce-d0592d23cda1"
   },
   "outputs": [],
   "source": [
    "n_importance = 3 #no of tags to be generated\n",
    "predict = importance_list(tf_idf_matrix,n_importance)\n",
    "predict_vs_actual = pd.DataFrame({'tags':predict,'id':raw['id']})\n",
    "predict_vs_actual['tags'] = predict_vs_actual['tags'].apply(\" \".join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "5a7f3d26-1127-8797-a51b-5d5cd8ba1e99"
   },
   "outputs": [],
   "source": [
    "predict_vs_actual.to_csv(\"predicted.csv\",index=False)\n",
    "\n",
    "print(predict_vs_actual[0:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9f3d4445-3158-c8be-bea9-0b5d7b06238a"
   },
   "source": [
    "This marks the end of this submission. Unfortunately, I was not able to implement transfer learning but, I was able to generate tags for all the questions."
   ]
  }
 ],
 "metadata": {
  "_change_revision": 3,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
