{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "225108d0-898e-9e00-5b7f-331df096a7d0"
   },
   "source": [
    "This is my submission for the task Tagging System of Questions using Transfer Learning. We use a kind of brute force method, using TF-IDF to predict the best possible tags for a question.\n",
    "\n",
    "\n",
    "First, load the required libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_cell_guid": "60e5123b-0d2e-faff-5148-b6246aa9c8d8"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import nltk # natural language processing\n",
    "import re # regular expression\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from string import punctuation\n",
    "\n",
    "stop = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_cell_guid": "f81d7e82-fd33-30dc-6d75-e049fa9aff96"
   },
   "outputs": [],
   "source": [
    "# read in test data sets\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "raw = test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "17b1d0a6-d27c-d3d9-2868-fea5a034d3f9"
   },
   "source": [
    "First, we perform data cleansing on our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a2fe2f86-f49b-6eb9-e4b4-bbb9fa769760"
   },
   "source": [
    "The content column contains some HTML tags. We clean our data to remove these tags, convert all to small case letters and remove stopwords. We also tokenize the data here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_cell_guid": "3727ba05-f6f4-495e-257f-0a77c3344240"
   },
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]p') \n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]') # take all words that contain characters other than 0-9,a-z,#,+\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def parse_content(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    #text = # lowercase text\n",
    "    text =text.lower()\n",
    "    #text = # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = re.sub(REPLACE_BY_SPACE_RE, ' ', text)\n",
    "    #text = # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text =  re.sub(BAD_SYMBOLS_RE, '', text)\n",
    "    text = re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "    text = re.sub(\"\\d+\", \"\", text)\n",
    "    word_tokens = word_tokenize(text) \n",
    "  \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stopwords]\n",
    "    text = filtered_sentence\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_cell_guid": "df4f6289-3856-8c5b-aff7-81953567f561"
   },
   "outputs": [],
   "source": [
    "# Apply parse_content onto dataframe.\n",
    "raw['content'] = raw['content'].apply(parse_content)\n",
    "raw['title'] = raw['title'].apply(parse_content)\n",
    "\n",
    "def importance_list_row(sparse_row,n_importance):\n",
    "    importance_list = [0]*n_importance\n",
    "    for i in range(0,n_importance): \n",
    "        ind =  sparse_row.indices[sparse_row.data.argmax(axis=0)] if sparse_row.nnz else 0\n",
    "        importance_list[i] = pos_to_word[ind]\n",
    "        sparse_row[0,ind] = 0\n",
    "    return importance_list\n",
    "\n",
    "\n",
    "def importance_list(sparse_matrix,n_importance):\n",
    "    n_row = sparse_matrix.shape[0]\n",
    "    importance_lists = [0]*n_row\n",
    "    for row in range(0,n_row):\n",
    "        importance_lists[row] = importance_list_row(sparse_matrix[row],n_importance)\n",
    "    return importance_lists   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7cc8261c-5d03-154a-81c5-6c28ba540d70"
   },
   "source": [
    "We do further processing on our text by selecting nouns which match a certain patters, and converting the entire document into tokens of words.\n",
    "After that, we perform lemmetization of the document.\n",
    "\n",
    "Now, we combine the title and content fields of input data, as we will be generating TF-IDF matrix using both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_cell_guid": "c4987560-82df-5625-b7bf-d960c3543304"
   },
   "outputs": [],
   "source": [
    "#tokenize and tag texts. \n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "raw['text_token'] = ' '\n",
    "raw['text_token'] = raw['title'] + raw['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_cell_guid": "c4987560-82df-5625-b7bf-d960c3543304"
   },
   "outputs": [],
   "source": [
    "raw['text_token'] = raw['text_token'].apply(lambda x:[lemmatizer.lemmatize(t) for t in x])\n",
    "raw['text_pos'] = raw['text_token'].apply(nltk.pos_tag)\n",
    "raw['text_nouns'] = raw['text_pos'].apply(lambda x: [pair[0] for pair in x if pair[1] in (\"NN\",\"NNS\",\"JJ\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_cell_guid": "e6c3f779-8c69-8570-0ea9-26b707b2931e"
   },
   "outputs": [],
   "source": [
    "raw['text_bigram'] = raw['text_pos'].apply(nltk.bigrams)\n",
    "raw['text_bigram'] = raw['text_bigram'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "_cell_guid": "ee749d56-1533-a076-f249-6d581d0de057"
   },
   "outputs": [],
   "source": [
    "def findPair(l):\n",
    "    result = []\n",
    "    for pair in l:\n",
    "        if pair[1][1] in ('NN','NNS') and pair[0][1] in ('NN','NNS','JJ'):\n",
    "            result.append(pair[0][0]+\" \"+pair[1][0])\n",
    "    return result\n",
    "raw['word_pair'] = raw['text_bigram'].apply(findPair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "597f9306-cfb6-b826-fd0e-8905551d3424"
   },
   "source": [
    "Now, lets make the actual predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_cell_guid": "c856254e-b879-6250-9947-3ed95bc2ec53"
   },
   "outputs": [],
   "source": [
    "mydoclist = raw['text_nouns'].apply(\" \".join).tolist()\n",
    "count_vectorizer = CountVectorizer(stop_words='english',lowercase=True,analyzer='word',ngram_range=(1,1))\n",
    "term_freq_matrix = count_vectorizer.fit_transform(mydoclist)\n",
    "tfidf = TfidfTransformer(norm=\"l2\")\n",
    "tfidf.fit(term_freq_matrix)\n",
    "tf_idf_matrix = tfidf.transform(term_freq_matrix)\n",
    "pos_to_word = dict([[v,k] for k,v in count_vectorizer.vocabulary_.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_cell_guid": "03354ab4-c6ed-98d2-e5ce-d0592d23cda1"
   },
   "outputs": [],
   "source": [
    "n_importance = 3 #no of tags to be generated\n",
    "predict = importance_list(tf_idf_matrix,n_importance)\n",
    "predict_vs_actual = pd.DataFrame({'tags':predict,'id':raw['id']})\n",
    "predict_vs_actual['tags'] = predict_vs_actual['tags'].apply(\" \".join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "_cell_guid": "5a7f3d26-1127-8797-a51b-5d5cd8ba1e99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 tags   id\n",
      "0                             spin subatomic particle    1\n",
      "1                                plausible theory non    2\n",
      "2                            group lie representation    3\n",
      "3                        determinism overcome laplace    7\n",
      "4                      hamilton phamiltons stationary    9\n",
      "5                                     sound clue life   13\n",
      "6                       theory thinkliliand ollithose   15\n",
      "7                                sky sunriseset night   17\n",
      "8                               energy collision hows   19\n",
      "9                                  monte carlo pwhere   21\n",
      "10                                    bike wheel turn   24\n",
      "11                   projectile youmichael vanderpool   26\n",
      "12                      particle measurement collapse   27\n",
      "13                                   mph average trip   29\n",
      "14  hrefhttpenwikipediaorgwikispecial_relativity r...   31\n",
      "15                       whirlvortex sinkbathtub myth   32\n",
      "16                                 magnet energy pole   35\n",
      "17                     theory world generalrelativity   37\n",
      "18                             physicist field theory   41\n",
      "19                    capacitive touchpad touchscreen   49\n",
      "20                                  pole theyd magnet   52\n",
      "21                                lhc circular geneva   62\n",
      "22                              polarised stress tape   68\n",
      "23                          phase tem relnofollowgouy   71\n",
      "24                           treatment therapy cancer   72\n",
      "25                  yangbaxter solution mathematician   73\n",
      "26                    mnemonic resistant ullihardness   75\n",
      "27                              neutron repel teacher   78\n",
      "28                        epr experiment entanglement   79\n",
      "29                             laser uncertainty shot   83\n",
      "..                                                ...  ...\n",
      "70  adult insightfulimpressive hysicsstackexchange...  218\n",
      "71              software strongfreestrong calculation  222\n",
      "72                            gyroscope aerotrim axis  228\n",
      "73                          mathematics student study  234\n",
      "74              element theyre ositroniumpositroniuma  236\n",
      "75                            irregular lab nonhollow  237\n",
      "76                                  float iron object  239\n",
      "77                              manybody problem body  252\n",
      "78                   accelerator synchrotron collider  257\n",
      "79                               domino tippage angle  258\n",
      "80                       vegetation capillary upwards  268\n",
      "81                                clothing white wear  270\n",
      "82                      explanation precession spiral  271\n",
      "83                   counterintuitive good relativity  272\n",
      "84                           arent intuitive rotation  286\n",
      "85                            airplane lift bernoulli  290\n",
      "86                              energy learned caveat  296\n",
      "87                model subgridscale parametrisations  300\n",
      "88                                water sound disturb  303\n",
      "89                          pitch determines firework  304\n",
      "90  photoelectric radiationstrike radiationlilistr...  305\n",
      "91                         physic pmathematical rigor  312\n",
      "92                         measurement outcome single  317\n",
      "93                          centripetal blanket cause  321\n",
      "94       angular momentum srchttpistackimgurcomxhtpng  326\n",
      "95          srchttpistackimgurcomakppng altalt number  328\n",
      "96                              precession period rad  331\n",
      "97                    electricity instantaneous delay  335\n",
      "98                        coefficienta ray refraction  339\n",
      "99                               peephole person door  340\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "predict_vs_actual.to_csv(\"predicted.csv\",index=False)\n",
    "\n",
    "print(predict_vs_actual[0:100]) # print results for first 100 questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9f3d4445-3158-c8be-bea9-0b5d7b06238a"
   },
   "source": [
    "This marks the end of this submission. Unfortunately, I was not able to implement transfer learning but, I was able to generate tags for all the questions."
   ]
  }
 ],
 "metadata": {
  "_change_revision": 3,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
