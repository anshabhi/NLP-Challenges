{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "225108d0-898e-9e00-5b7f-331df096a7d0"
   },
   "source": [
    "This doc will walk through Three stages of studying stackflow tags data set.\n",
    "\n",
    " **1. First stage: Data cleaning**\n",
    "\n",
    "As we notice there are columns that contain html mark ups, we want to make sure that it is taken care of and cleaned up\n",
    "\n",
    " **2. Second Stage: Feature engineering**\n",
    "\n",
    "Apply methods to engineer features from cleaned text data\n",
    " \n",
    "**3. Third Stage: Classification modeling**\n",
    "\n",
    "Using engineered features to build predictive models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "60e5123b-0d2e-faff-5148-b6246aa9c8d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biology.csv\n",
      "cooking.csv\n",
      "crypto.csv\n",
      "diy.csv\n",
      "robotics.csv\n",
      "sample_submission.csv\n",
      "test.csv\n",
      "travel.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import nltk # natural language processing\n",
    "import re # regular expression\n",
    "from bs4 import BeautifulSoup #scraping HTML\n",
    "from nltk.corpus import stopwords\n",
    "import seaborn as sns # visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from string import punctuation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# nltk workspace\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "f81d7e82-fd33-30dc-6d75-e049fa9aff96"
   },
   "outputs": [],
   "source": [
    "# read in raw data sets\n",
    "biology = pd.read_csv(\"../input/biology.csv\")\n",
    "cooking = pd.read_csv(\"../input/cooking.csv\")\n",
    "crypto = pd.read_csv(\"../input/crypto.csv\")\n",
    "diy = pd.read_csv(\"../input/diy.csv\")\n",
    "robotics = pd.read_csv(\"../input/robotics.csv\")\n",
    "travel = pd.read_csv(\"../input/travel.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "a92bfbd0-08d4-8592-300d-050603b037f1"
   },
   "outputs": [],
   "source": [
    "## Concatenate datasets\n",
    "#raw = pd.concat([biology,cooking,crypto,diy,robotics,travel],axis = 0, ignore_index =True)\n",
    "raw = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "e3f3d6cc-5647-f230-b018-9e6ccf8892f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset has in total 81926 rows.\n"
     ]
    }
   ],
   "source": [
    "# Simple Statistics\n",
    "print(\"This dataset has in total {} rows.\".format(raw.shape[0]))\n",
    "#print(\"out of which, {} rows come from train dataset and {} rows come from test dataset.\".format(raw[raw['source']=='train'].shape[0],raw[raw['source']=='test'].shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "17b1d0a6-d27c-d3d9-2868-fea5a034d3f9"
   },
   "source": [
    "# **Stage 1: Data Cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a2fe2f86-f49b-6eb9-e4b4-bbb9fa769760"
   },
   "source": [
    "Content column is a little bit messy since it's embedded in HTML. cleaning it up before applying any natural language processing tactics will facilitates our analysis moving forward. Beautifulsoup is a python package that helps dealing with HTML. The primary goal of this exercise is to:\n",
    "\n",
    "\n",
    " - Getting the clean text data;\n",
    " - Extract potentially important information from HTML tags. \n",
    "\n",
    " \n",
    "**Tags we want to pay attention to:**\n",
    "\n",
    " 1. The emphasize class: strong, em, i, li\n",
    " 2. The header class: h1, h2, h3\n",
    " 3. Link "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "3727ba05-f6f4-495e-257f-0a77c3344240"
   },
   "outputs": [],
   "source": [
    "def parse_content(s):\n",
    "    emphasize = []\n",
    "    header = []\n",
    "    link = []\n",
    "    content = \"\"\n",
    "    soup = BeautifulSoup(s,'html.parser')\n",
    "    content = soup.get_text()\n",
    "    return pd.Series({'content_parsed':content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "df4f6289-3856-8c5b-aff7-81953567f561"
   },
   "outputs": [],
   "source": [
    "# Apply parse_content onto dataframe.\n",
    "raw = pd.concat([raw,raw['content'].apply(parse_content)],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "17b7d828-5ca8-32ab-98c7-efc7398f9713"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>What is your simplest explanation of the strin...</td>\n",
       "      <td>&lt;p&gt;How would you explain string theory to non ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Lie theory, Representations and particle physics</td>\n",
       "      <td>&lt;p&gt;This is a question that has been posted at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>Will Determinism be ever possible?</td>\n",
       "      <td>&lt;p&gt;What are the main problems that we need to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>Hamilton's Principle</td>\n",
       "      <td>&lt;p&gt;Hamilton's principle states that a dynamic ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title  \\\n",
       "1   2  What is your simplest explanation of the strin...   \n",
       "2   3   Lie theory, Representations and particle physics   \n",
       "3   7                 Will Determinism be ever possible?   \n",
       "4   9                               Hamilton's Principle   \n",
       "\n",
       "                                             content  \n",
       "1  <p>How would you explain string theory to non ...  \n",
       "2  <p>This is a question that has been posted at ...  \n",
       "3  <p>What are the main problems that we need to ...  \n",
       "4  <p>Hamilton's principle states that a dynamic ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "74f04450-b588-bab0-7db4-d08c4b14a89a"
   },
   "source": [
    "## Stage 2: A dive into the tags##\n",
    "\n",
    "Before we predict the tags, we want to study and understand the tags. Two initial thoughts are:\n",
    "\n",
    " - Studying the distribution of word counts \n",
    " - Studying the distribution of semantics counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "bfeea021-89da-6847-63ea-ed6fb1b1c72c"
   },
   "outputs": [],
   "source": [
    "## Study the tags Word count distribution\n",
    "#raw['tags'] = raw['tags'].apply(lambda x: x.split(\" \"))\n",
    "#raw['tags_wc'] = raw['tags'].apply(len)\n",
    "#sns.barplot(x='tags_wc',y='tags_wc',data=raw,estimator=lambda x: len(x),palette='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "7ffcd0d8-5111-5c04-cf0e-6d4963aabc54"
   },
   "outputs": [],
   "source": [
    "##study the tags: semantic structure\n",
    "#raw['tags_token'] = raw['tags'].apply(str).apply(nltk.word_tokenize)\n",
    "#raw['tags_pos'] = raw['tags'].apply(nltk.pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "7773d5ca-55d2-b217-1d3a-5d6e2ff8611d"
   },
   "outputs": [],
   "source": [
    "# What is the distribution of different semantics?\n",
    "#semantics = pd.DataFrame({'semantics' : [pair[1] for col in raw['tags_pos'].tolist() for pair in col]})\n",
    "#semantics['count'] = 1\n",
    "#fig,axs = plt.subplots()\n",
    "#sns.barplot(x='semantics',y='count',data=semantics,estimator=lambda x: len(x),palette=sns.cubehelix_palette(8, start=.5, rot=-.75),ax=axs)\n",
    "#axs.set_title('semantic distribution')\n",
    "#axs.set_ylabel('frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "108c9d98-2b02-2db1-c186-bb67f6f47562"
   },
   "source": [
    "As what we would expect, The semantic strongly skewed towards **nouns** (NN,NNS,JJ)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9a5a14d0-10a3-17a5-7e4f-8a3f488f896c"
   },
   "source": [
    "## **Naïve Try : Rule based Algorithm** ##\n",
    "\n",
    "The first Iteration is very brute. We are going to count the frequencies(but here \"adjusted\" frequencies - TF-IDF), the highest the score one word get, the more likely it's going to be included in the tags.\n",
    "\n",
    "How many words are we going to include? Since the number of words a tag can have has an upper bound 5, we are going to include 5 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "0a7be3a3-70d3-7235-cbdc-2195e3f05d8a"
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "def strip_punctuation(s):\n",
    "    return ''.join(c for c in s if c not in punctuation)\n",
    "#Building NLTK pipelines\n",
    "def td_idf_matrix(dataset):\n",
    "    dataset['all_text'] = dataset['title'] + dataset['content_parsed'] \n",
    "    dataset['all_text'] = dataset['all_text'].apply(lambda x: str.lower(x).replace('\\n',' '))\n",
    "    mydoclist = [strip_punctuation(doc) for doc in dataset['all_text'].tolist()]\n",
    "    count_vectorizer = CountVectorizer(stop_words='english',lowercase=True,analyzer='word')\n",
    "    term_freq_matrix = count_vectorizer.fit_transform(mydoclist)\n",
    "    tfidf = TfidfTransformer(norm=\"l2\")\n",
    "    tfidf.fit(term_freq_matrix)\n",
    "    tf_idf_matrix = tfidf.transform(term_freq_matrix)\n",
    "    pos_to_word = dict([[v,k] for k,v in count_vectorizer.vocabulary_.items()])\n",
    "    return tf_idf_matrix, pos_to_word\n",
    "\n",
    "tf_idf_matrix, pos_to_word = td_idf_matrix(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "3cc35f39-fec4-82ca-f3a8-132888df640a"
   },
   "outputs": [],
   "source": [
    "\n",
    "def importance_list_row(sparse_row,n_importance):\n",
    "    importance_list = [0]*n_importance\n",
    "    for i in range(0,n_importance): \n",
    "        ind =  sparse_row.indices[sparse_row.data.argmax(axis=0)] if sparse_row.nnz else 0\n",
    "        importance_list[i] = pos_to_word[ind]\n",
    "        sparse_row[0,ind] = 0\n",
    "    return importance_list\n",
    "\n",
    "\n",
    "def importance_list(sparse_matrix,n_importance):\n",
    "    n_row = sparse_matrix.shape[0]\n",
    "    importance_lists = [0]*n_row\n",
    "    for row in range(0,n_row):\n",
    "        importance_lists[row] = importance_list_row(sparse_matrix[row],n_importance)\n",
    "    return importance_lists   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "371acc19-cf65-871b-d49d-0ba699249bb7"
   },
   "outputs": [],
   "source": [
    "#n_importance = 2\n",
    "#predict = importance_list(tf_idf_matrix,n_importance)\n",
    "#predict_vs_actual = pd.DataFrame({'predict':predict})\n",
    "#predict_vs_actual['predict'] = predict_vs_actual['predict'].apply(lambda x: \"\".join(chr+\" \") for char in x)\n",
    "#predict_vs_actual[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7cc8261c-5d03-154a-81c5-6c28ba540d70"
   },
   "source": [
    "The result from our naive approach is quite nasty. For the next iteration, I will work on improving it.\n",
    "\n",
    "One idea stems from the semantic distribution. The majority of tags are nouns and adjectives. therefore I'm going to tag my content and parse out words by their semantics and only focus on the **noun and the adjectives**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "c4987560-82df-5625-b7bf-d960c3543304"
   },
   "outputs": [],
   "source": [
    "#tokenize and tag texts. \n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "raw['all_text'] = raw['all_text'].apply(strip_punctuation)\n",
    "raw['text_token'] = raw['all_text'].apply(nltk.word_tokenize)\n",
    "raw['text_token'] = raw['all_text'].apply(nltk.word_tokenize)\n",
    "raw['text_token'] = raw['text_token'].apply(lambda x:[lemmatizer.lemmatize(t) for t in x])\n",
    "raw['text_pos'] = raw['text_token'].apply(nltk.pos_tag)\n",
    "raw['text_nouns'] = raw['text_pos'].apply(lambda x: [pair[0] for pair in x if pair[1] in (\"NN\",\"NNS\",\"JJ\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "e6c3f779-8c69-8570-0ea9-26b707b2931e"
   },
   "outputs": [],
   "source": [
    "raw['text_bigram'] = raw['text_pos'].apply(nltk.bigrams)\n",
    "raw['text_bigram'] = raw['text_bigram'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "520c9850-aa1f-1751-caa7-a94f820dfb66"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'findPair' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-05ab5c806d9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mraw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word_pair'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_bigram'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfindPair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'findPair' is not defined"
     ]
    }
   ],
   "source": [
    "raw['word_pair'] = raw['text_bigram'].apply(findPair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "f1ce7e27-8032-2b3f-d90a-cd069ab02ac9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>content_parsed</th>\n",
       "      <th>all_text</th>\n",
       "      <th>text_token</th>\n",
       "      <th>text_pos</th>\n",
       "      <th>text_nouns</th>\n",
       "      <th>text_bigram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>What is spin as it relates to subatomic partic...</td>\n",
       "      <td>&lt;p&gt;I often hear about subatomic particles havi...</td>\n",
       "      <td>I often hear about subatomic particles having ...</td>\n",
       "      <td>what is spin as it relates to subatomic partic...</td>\n",
       "      <td>[what, is, spin, a, it, relates, to, subatomic...</td>\n",
       "      <td>[(what, WP), (is, VBZ), (spin, VBG), (a, DT), ...</td>\n",
       "      <td>[particlesi, subatomic, particle, property, sp...</td>\n",
       "      <td>[((what, WP), (is, VBZ)), ((is, VBZ), (spin, V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>What is your simplest explanation of the strin...</td>\n",
       "      <td>&lt;p&gt;How would you explain string theory to non ...</td>\n",
       "      <td>How would you explain string theory to non phy...</td>\n",
       "      <td>what is your simplest explanation of the strin...</td>\n",
       "      <td>[what, is, your, simplest, explanation, of, th...</td>\n",
       "      <td>[(what, WP), (is, VBZ), (your, PRP$), (simples...</td>\n",
       "      <td>[simplest, explanation, string, theoryhow, the...</td>\n",
       "      <td>[((what, WP), (is, VBZ)), ((is, VBZ), (your, P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Lie theory, Representations and particle physics</td>\n",
       "      <td>&lt;p&gt;This is a question that has been posted at ...</td>\n",
       "      <td>This is a question that has been posted at man...</td>\n",
       "      <td>lie theory representations and particle physic...</td>\n",
       "      <td>[lie, theory, representation, and, particle, p...</td>\n",
       "      <td>[(lie, NN), (theory, NN), (representation, NN)...</td>\n",
       "      <td>[lie, theory, representation, particle, physic...</td>\n",
       "      <td>[((lie, NN), (theory, NN)), ((theory, NN), (re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>Will Determinism be ever possible?</td>\n",
       "      <td>&lt;p&gt;What are the main problems that we need to ...</td>\n",
       "      <td>What are the main problems that we need to sol...</td>\n",
       "      <td>will determinism be ever possiblewhat are the ...</td>\n",
       "      <td>[will, determinism, be, ever, possiblewhat, ar...</td>\n",
       "      <td>[(will, MD), (determinism, VB), (be, VB), (eve...</td>\n",
       "      <td>[main, problem, laplace, determinism, correct,...</td>\n",
       "      <td>[((will, MD), (determinism, VB)), ((determinis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>Hamilton's Principle</td>\n",
       "      <td>&lt;p&gt;Hamilton's principle states that a dynamic ...</td>\n",
       "      <td>Hamilton's principle states that a dynamic sys...</td>\n",
       "      <td>hamiltons principlehamiltons principle states ...</td>\n",
       "      <td>[hamilton, principlehamiltons, principle, stat...</td>\n",
       "      <td>[(hamilton, NN), (principlehamiltons, NNS), (p...</td>\n",
       "      <td>[hamilton, principlehamiltons, state, dynamic,...</td>\n",
       "      <td>[((hamilton, NN), (principlehamiltons, NNS)), ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title  \\\n",
       "0   1  What is spin as it relates to subatomic partic...   \n",
       "1   2  What is your simplest explanation of the strin...   \n",
       "2   3   Lie theory, Representations and particle physics   \n",
       "3   7                 Will Determinism be ever possible?   \n",
       "4   9                               Hamilton's Principle   \n",
       "\n",
       "                                             content  \\\n",
       "0  <p>I often hear about subatomic particles havi...   \n",
       "1  <p>How would you explain string theory to non ...   \n",
       "2  <p>This is a question that has been posted at ...   \n",
       "3  <p>What are the main problems that we need to ...   \n",
       "4  <p>Hamilton's principle states that a dynamic ...   \n",
       "\n",
       "                                      content_parsed  \\\n",
       "0  I often hear about subatomic particles having ...   \n",
       "1  How would you explain string theory to non phy...   \n",
       "2  This is a question that has been posted at man...   \n",
       "3  What are the main problems that we need to sol...   \n",
       "4  Hamilton's principle states that a dynamic sys...   \n",
       "\n",
       "                                            all_text  \\\n",
       "0  what is spin as it relates to subatomic partic...   \n",
       "1  what is your simplest explanation of the strin...   \n",
       "2  lie theory representations and particle physic...   \n",
       "3  will determinism be ever possiblewhat are the ...   \n",
       "4  hamiltons principlehamiltons principle states ...   \n",
       "\n",
       "                                          text_token  \\\n",
       "0  [what, is, spin, a, it, relates, to, subatomic...   \n",
       "1  [what, is, your, simplest, explanation, of, th...   \n",
       "2  [lie, theory, representation, and, particle, p...   \n",
       "3  [will, determinism, be, ever, possiblewhat, ar...   \n",
       "4  [hamilton, principlehamiltons, principle, stat...   \n",
       "\n",
       "                                            text_pos  \\\n",
       "0  [(what, WP), (is, VBZ), (spin, VBG), (a, DT), ...   \n",
       "1  [(what, WP), (is, VBZ), (your, PRP$), (simples...   \n",
       "2  [(lie, NN), (theory, NN), (representation, NN)...   \n",
       "3  [(will, MD), (determinism, VB), (be, VB), (eve...   \n",
       "4  [(hamilton, NN), (principlehamiltons, NNS), (p...   \n",
       "\n",
       "                                          text_nouns  \\\n",
       "0  [particlesi, subatomic, particle, property, sp...   \n",
       "1  [simplest, explanation, string, theoryhow, the...   \n",
       "2  [lie, theory, representation, particle, physic...   \n",
       "3  [main, problem, laplace, determinism, correct,...   \n",
       "4  [hamilton, principlehamiltons, state, dynamic,...   \n",
       "\n",
       "                                         text_bigram  \n",
       "0  [((what, WP), (is, VBZ)), ((is, VBZ), (spin, V...  \n",
       "1  [((what, WP), (is, VBZ)), ((is, VBZ), (your, P...  \n",
       "2  [((lie, NN), (theory, NN)), ((theory, NN), (re...  \n",
       "3  [((will, MD), (determinism, VB)), ((determinis...  \n",
       "4  [((hamilton, NN), (principlehamiltons, NNS)), ...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "ee749d56-1533-a076-f249-6d581d0de057"
   },
   "outputs": [],
   "source": [
    "def findPair(l):\n",
    "    result = []\n",
    "    for pair in l:\n",
    "        if pair[1][1] in ('NN','NNS') and pair[0][1] in ('NN','NNS','JJ'):\n",
    "            result.append(pair[0][0]+\" \"+pair[1][0])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "597f9306-cfb6-b826-fd0e-8905551d3424"
   },
   "source": [
    "This seems to look much better. Let's give it a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "c856254e-b879-6250-9947-3ed95bc2ec53"
   },
   "outputs": [],
   "source": [
    "mydoclist = raw['text_nouns'].apply(\" \".join).tolist()\n",
    "#mydoclist[0:5]\n",
    "count_vectorizer = CountVectorizer(stop_words='english',lowercase=True,analyzer='word',ngram_range=(1,1))\n",
    "term_freq_matrix = count_vectorizer.fit_transform(mydoclist)\n",
    "tfidf = TfidfTransformer(norm=\"l2\")\n",
    "tfidf.fit(term_freq_matrix)\n",
    "tf_idf_matrix = tfidf.transform(term_freq_matrix)\n",
    "pos_to_word = dict([[v,k] for k,v in count_vectorizer.vocabulary_.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "03354ab4-c6ed-98d2-e5ce-d0592d23cda1"
   },
   "outputs": [],
   "source": [
    "n_importance = 3\n",
    "predict = importance_list(tf_idf_matrix,n_importance)\n",
    "predict_vs_actual = pd.DataFrame({'tags':predict,'id':raw['id']})\n",
    "predict_vs_actual['tags'] = predict_vs_actual['tags'].apply(\" \".join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "7378d643-96dc-3cff-c97f-962b320ecc2c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>spin particlesi spinning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>theoryhow simplest plausible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>group lie representation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>determinism laplace main</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>principlehamiltons stationary action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13</td>\n",
       "      <td>sound producedive clue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15</td>\n",
       "      <td>string theory group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>17</td>\n",
       "      <td>sky sunriseset blue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>19</td>\n",
       "      <td>energy collision calculatedphysicists</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>21</td>\n",
       "      <td>monte carlo method</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>24</td>\n",
       "      <td>bike wheel turn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>26</td>\n",
       "      <td>projectile vanderpool electromagnetic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>27</td>\n",
       "      <td>particle measurement collapse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>29</td>\n",
       "      <td>average trip mile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>31</td>\n",
       "      <td>lay special theory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>32</td>\n",
       "      <td>whirlvortex sinkbathtubthere myth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>35</td>\n",
       "      <td>magnet energy pole</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>37</td>\n",
       "      <td>theory einsteinlike worldphysicists</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>41</td>\n",
       "      <td>physicist field theory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>49</td>\n",
       "      <td>capacitive touchpad touchscreen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>52</td>\n",
       "      <td>pole repel magnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>62</td>\n",
       "      <td>lhc longthe circular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>68</td>\n",
       "      <td>polarised tape colour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>71</td>\n",
       "      <td>gouy phase tem01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>72</td>\n",
       "      <td>therapy cancer treatment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>73</td>\n",
       "      <td>yangbaxter solution mathematician</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>75</td>\n",
       "      <td>mnemonic resistant failure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>78</td>\n",
       "      <td>teacher neutron otheri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>79</td>\n",
       "      <td>entangled experiment entanglement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>83</td>\n",
       "      <td>uncertainty shot noise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>218</td>\n",
       "      <td>insightfulimpressive adultssimilar adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>222</td>\n",
       "      <td>software calculationswhat symbolic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>228</td>\n",
       "      <td>ring aerotrim gyroscope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>234</td>\n",
       "      <td>mathematics student curriculum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>236</td>\n",
       "      <td>positronium element theyre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>237</td>\n",
       "      <td>irregular lab nonhollow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>239</td>\n",
       "      <td>object iron friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>252</td>\n",
       "      <td>manybody quantum body</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>257</td>\n",
       "      <td>accelerator synchrotron tevatron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>258</td>\n",
       "      <td>domino tippage angle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>268</td>\n",
       "      <td>capillary pullpress vegetationeach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>270</td>\n",
       "      <td>clothing wear white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>271</td>\n",
       "      <td>explanation precession diagram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>272</td>\n",
       "      <td>counterintuitive physicsi good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>286</td>\n",
       "      <td>arentdoes intuitive rotation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>290</td>\n",
       "      <td>wing airplane bernoulli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>296</td>\n",
       "      <td>energy conservedin caveat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>300</td>\n",
       "      <td>model subgridscale parametrisations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>303</td>\n",
       "      <td>water disturbedwhen sound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>304</td>\n",
       "      <td>pitch firework hertz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>305</td>\n",
       "      <td>radiation photoelectric electron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>312</td>\n",
       "      <td>physic startedi rigor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>317</td>\n",
       "      <td>measurement outcome single</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>321</td>\n",
       "      <td>centripetal blanket gravity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>326</td>\n",
       "      <td>angular momentum torque</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>328</td>\n",
       "      <td>number suggestion angular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>331</td>\n",
       "      <td>period precession precessionim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>335</td>\n",
       "      <td>electricity instantaneousmy delay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>339</td>\n",
       "      <td>ray coefficient refraction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>340</td>\n",
       "      <td>peephole person door</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                      tags\n",
       "0     1                  spin particlesi spinning\n",
       "1     2              theoryhow simplest plausible\n",
       "2     3                  group lie representation\n",
       "3     7                  determinism laplace main\n",
       "4     9      principlehamiltons stationary action\n",
       "5    13                    sound producedive clue\n",
       "6    15                       string theory group\n",
       "7    17                       sky sunriseset blue\n",
       "8    19     energy collision calculatedphysicists\n",
       "9    21                        monte carlo method\n",
       "10   24                           bike wheel turn\n",
       "11   26     projectile vanderpool electromagnetic\n",
       "12   27             particle measurement collapse\n",
       "13   29                         average trip mile\n",
       "14   31                        lay special theory\n",
       "15   32         whirlvortex sinkbathtubthere myth\n",
       "16   35                        magnet energy pole\n",
       "17   37       theory einsteinlike worldphysicists\n",
       "18   41                    physicist field theory\n",
       "19   49           capacitive touchpad touchscreen\n",
       "20   52                         pole repel magnet\n",
       "21   62                      lhc longthe circular\n",
       "22   68                     polarised tape colour\n",
       "23   71                          gouy phase tem01\n",
       "24   72                  therapy cancer treatment\n",
       "25   73         yangbaxter solution mathematician\n",
       "26   75                mnemonic resistant failure\n",
       "27   78                    teacher neutron otheri\n",
       "28   79         entangled experiment entanglement\n",
       "29   83                    uncertainty shot noise\n",
       "..  ...                                       ...\n",
       "70  218  insightfulimpressive adultssimilar adult\n",
       "71  222        software calculationswhat symbolic\n",
       "72  228                   ring aerotrim gyroscope\n",
       "73  234            mathematics student curriculum\n",
       "74  236                positronium element theyre\n",
       "75  237                   irregular lab nonhollow\n",
       "76  239                        object iron friend\n",
       "77  252                     manybody quantum body\n",
       "78  257          accelerator synchrotron tevatron\n",
       "79  258                      domino tippage angle\n",
       "80  268        capillary pullpress vegetationeach\n",
       "81  270                       clothing wear white\n",
       "82  271            explanation precession diagram\n",
       "83  272            counterintuitive physicsi good\n",
       "84  286              arentdoes intuitive rotation\n",
       "85  290                   wing airplane bernoulli\n",
       "86  296                 energy conservedin caveat\n",
       "87  300       model subgridscale parametrisations\n",
       "88  303                 water disturbedwhen sound\n",
       "89  304                      pitch firework hertz\n",
       "90  305          radiation photoelectric electron\n",
       "91  312                     physic startedi rigor\n",
       "92  317                measurement outcome single\n",
       "93  321               centripetal blanket gravity\n",
       "94  326                   angular momentum torque\n",
       "95  328                 number suggestion angular\n",
       "96  331            period precession precessionim\n",
       "97  335         electricity instantaneousmy delay\n",
       "98  339                ray coefficient refraction\n",
       "99  340                      peephole person door\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_vs_actual[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "5a7f3d26-1127-8797-a51b-5d5cd8ba1e99"
   },
   "outputs": [],
   "source": [
    "predict_vs_actual.to_csv(\"predicted.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9f3d4445-3158-c8be-bea9-0b5d7b06238a"
   },
   "source": [
    "# **Stage Two: Feature Engineering(TBD)**\n",
    "\n",
    "When people are posting on stack flow, they often have a **problem** to **solve**. So in the texts, what comes the most important would be the **noun** followed by the **verb**.\n",
    "\n",
    "As we are predicting unseen topics, supervised learning might not be deemed as useful here. For the first iteration I'm going to take a naïve approach - by building up a very simple, **rule based** algorithm.  \n",
    "\n",
    "A natural thought lands on frequency - But we also don't want to look at term frequency since we don't want our result overshadowed by high frequent but meaningless stop words. TF-IDF comes in handy.\n",
    "\n",
    "Also - from my years of experiences in writing irritatingly stupid questions on stack flow,  writing the core question in the title as well as in the content will increase the odds of getting your question answers. reversely, **if a keyword shows up both on titles and on content, it might be the one we are looking for**.\n",
    "\n",
    "Other considerations - we may want to study the tag a little bit to understand how many words people normally tend to include in the tag(the word count distribution of the tags) and whether their is evident semantic structure to the tags. "
   ]
  }
 ],
 "metadata": {
  "_change_revision": 3,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
